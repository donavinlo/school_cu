{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWgYXQW1kuSg"
   },
   "source": [
    "# MSDS Marketing Text Analytics, Unit 2, Assignment 2: Build a topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLHoXywD4Cqg"
   },
   "source": [
    "## ⚡️ Make a Copy\n",
    "\n",
    "Save a copy of this notebook in your Google Drive before continuing. Be sure to edit your own copy, not the original notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3YbHDKkk1vq"
   },
   "source": [
    "In this assignment, you will implement a topic model preprocessor which can then be applied to the task of topic-modeling Amazon text reviews. Please review the course lectures and documentation up to this point before continuing. Be sure also to be familiar with the [documentation for TMToolkit](https://tmtoolkit.readthedocs.io/en/latest/topic_modeling.html)\n",
    "\n",
    "Be sure to make a copy into your own Drive account before editing this notebook.\n",
    "\n",
    "You will implement a preprocessing function to prepare your corpus for topic modeling. It is recommended that you use a small test corpus (an example is provided below) for development, rather than starting with the full review set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Rfw11JBiZHi"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jpxYdJGQiWqj",
    "outputId": "7bef0172-d4c2-4d9c-e3f3-7d2f899fb1bd"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'require_listlike' from 'tmtoolkit.utils' (/opt/anaconda3/lib/python3.9/site-packages/tmtoolkit/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtmtoolkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Corpus\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtmtoolkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TMPreproc\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtmtoolkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtopicmod\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_io\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_ldamodel_topic_words\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtmtoolkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtopicmod\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtm_lda\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_models_parallel\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tmtoolkit/preprocess/__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m Doc\u001b[38;5;241m.\u001b[39mset_extension(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     DEFAULT_LANGUAGE_MODELS, LANGUAGE_LABELS, load_stopwords, simplified_pos\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_docfuncs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     init_for_language, tokenize, doc_labels, doc_tokens, doc_lengths, doc_frequencies, vocabulary, vocabulary_counts,\n\u001b[1;32m     13\u001b[0m     ngrams, sparse_dtm, kwic, kwic_table, glue_tokens, expand_compounds, lemmatize, pos_tag, pos_tags, clean_tokens,\n\u001b[1;32m     14\u001b[0m     compact_documents, filter_tokens, filter_tokens_by_mask, filter_tokens_with_kwic, filter_for_pos,\n\u001b[1;32m     15\u001b[0m     filter_documents_by_name, filter_documents, remove_tokens, remove_tokens_by_mask, remove_documents,\n\u001b[1;32m     16\u001b[0m     remove_documents_by_name, remove_tokens_by_doc_frequency, remove_common_tokens, remove_uncommon_tokens,\n\u001b[1;32m     17\u001b[0m     tokendocs2spacydocs, spacydoc_from_tokens, transform, to_lowercase, remove_chars, tokens2ids, ids2tokens\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tokenfuncs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     token_match, token_match_subsequent, token_glue_subsequent, expand_compound_token,\n\u001b[1;32m     22\u001b[0m     str_shape, str_shapesplit, str_multisplit,\n\u001b[1;32m     23\u001b[0m     make_index_window_around_matches\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# when NLTK is installed\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tmtoolkit/preprocess/_docfuncs.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocab\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_LANGUAGE_MODELS, load_stopwords, simplified_pos\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tokenfuncs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     require_tokendocs, token_match, token_match_subsequent, token_glue_subsequent, make_index_window_around_matches,\n\u001b[1;32m     18\u001b[0m     expand_compound_token\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_listlike, require_types, flatten_list, empty_chararray, widen_chararray\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_sparse_dtm\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tmtoolkit/preprocess/_tokenfuncs.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglobre\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_listlike, require_listlike_or_set, flatten_list, empty_chararray\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#%%\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoken_match\u001b[39m(pattern, tokens, match_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexact\u001b[39m\u001b[38;5;124m'\u001b[39m, ignore_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, glob_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'require_listlike' from 'tmtoolkit.utils' (/opt/anaconda3/lib/python3.9/site-packages/tmtoolkit/utils.py)"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tmtoolkit.corpus import Corpus\n",
    "    from tmtoolkit.preprocess import TMPreproc\n",
    "    from tmtoolkit.topicmod.model_io import print_ldamodel_topic_words\n",
    "    from tmtoolkit.topicmod.tm_lda import compute_models_parallel\n",
    "except ModuleNotFoundError:\n",
    "    !pip install lda\n",
    "    !pip install tmtoolkit\n",
    "    from tmtoolkit.corpus import Corpus\n",
    "    from tmtoolkit.preprocess import TMPreproc\n",
    "    from tmtoolkit.topicmod.model_io import print_ldamodel_topic_words\n",
    "    from tmtoolkit.topicmod.tm_lda import compute_models_parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDKzLINMjD2n"
   },
   "source": [
    "**NOTE:** Loading a corpus as a list of strings is not the only way to use tmtoolkit. Given, for example, a large corpus that might not fit in memory, the current approach would not work well. See the tmtoolkit docs on [working with text corpora](https://tmtoolkit.readthedocs.io/en/latest/text_corpora.html) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8g9kSJwi7M7"
   },
   "source": [
    "## Implement a pre-processor\n",
    "\n",
    "Here you will implement a function called `preprocess` which returns the TMPreproc object to be used for topic modeling.\n",
    "\n",
    "The preprocess function will take a list of texts and return a pre-processed corpus object, i.e. a TMPreproc object. Preprocessing should include the following actions on the corpus using the appropriate methods in the TMPreproc class:\n",
    "\n",
    " - lemmatize the texts\n",
    " - convert tokens to lowercase\n",
    " - remove special characters\n",
    " - clean tokens to remove numbers and any tokens shorter than 3 characters\n",
    "\n",
    "The first part of the function to create the corpus and preprocess object are done for you. Your job is to call the specific preprocess functions and to return the resulting preprocess object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eQFevchKhohK"
   },
   "outputs": [],
   "source": [
    "def preprocess(texts, lang=\"en\"):\n",
    "    \"\"\"Preprocessor which returns a TMPreproc object processed on corpus as language\n",
    "    specified by lang (defaults to \"en\"):\n",
    "\n",
    "    Should perform all of the following pre-processing functions:\n",
    "     - lemmatize\n",
    "     - tokens_to_lowercase\n",
    "     - remove_special_chars_in_tokens\n",
    "     - clean_tokens (remove numbers, and remove tokens shorter than 2)\n",
    "    \"\"\"\n",
    "    # Here, we just use the index of the text as the label for the corpus item\n",
    "    corpus = Corpus({ i:r for i, r in enumerate(texts) })\n",
    "    preproc = TMPreproc(corpus, language=lang)\n",
    "\n",
    "    # TODO: Complete the implementation of this function and submit the\n",
    "    # .py download of this notebook as your assignment submission.\n",
    "    # lemmatize not working correctly\n",
    "\n",
    "    return preproc.lemmatize().tokens_to_lowercase().remove_special_chars_in_tokens().clean_tokens(remove_shorter_than=2, remove_numbers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4W99eaBwldp3"
   },
   "outputs": [],
   "source": [
    "# %pip install -Iv spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DyxZzrM6aakv",
    "outputId": "fe237b59-35dd-4706-f0f0-743739a4a969"
   },
   "outputs": [],
   "source": [
    "help(TMPreproc.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_jJg5-xRczD"
   },
   "outputs": [],
   "source": [
    "#~~ /autograde # do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2aoBerYnF2M"
   },
   "source": [
    "---\n",
    "### ⚠️  **Caution:** No arbitrary code above this line\n",
    "\n",
    "The only code written above should be the implementation of your graded function. For experimentation and testing, only add code below.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBlUJqEan3oc"
   },
   "source": [
    "## Function development\n",
    "\n",
    "Use this section of code to verify your function implementation. You may change the test_corpus as needed to verify your implementation. The grader will be checking that your function returns a TMPreproc object that meets all of the following critera:\n",
    "\n",
    " - tokens are lemmatized\n",
    " - tokens are converted to lowercase\n",
    " - special characters are removed from tokens\n",
    " - tokens shorter than 3 characters and numerics are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YBlpS0RsrJ3s"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "_tk2oDy5j1gQ",
    "outputId": "2ad74972-4818-41db-d202-fe2bb860dea3"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "either `language`, `language_model` or `spacy_instance` must be given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m test_corpus \u001b[38;5;241m=\u001b[39m [ \u001b[38;5;66;03m# Feel free to edit this corpus for further testing\u001b[39;00m\n\u001b[1;32m      2\u001b[0m                 \u001b[38;5;66;03m# to be sure that your functions meet specifications.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe 3 cats sat on the mats!\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1 fish 2 fish Red fish Blue fish\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShe sells $ea$shells\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0m preproc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_corpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m pp\u001b[38;5;241m.\u001b[39mpprint(preproc\u001b[38;5;241m.\u001b[39mget_tokens())\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(texts, lang)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"Preprocessor which returns a TMPreproc object processed on corpus as language\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mspecified by lang (defaults to \"en\"):\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m - clean_tokens (remove numbers, and remove tokens shorter than 2)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Here, we just use the index of the text as the label for the corpus item\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[43mCorpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m preproc \u001b[38;5;241m=\u001b[39m TMPreproc(corpus, language\u001b[38;5;241m=\u001b[39mlang)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# TODO: Complete the implementation of this function and submit the\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# .py download of this notebook as your assignment submission.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# lemmatize not working correctly\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tmtoolkit/corpus/_corpus.py:190\u001b[0m, in \u001b[0;36mCorpus.__init__\u001b[0;34m(self, docs, language, language_model, load_features, add_features, raw_preproc, spacy_token_attrs, spacy_instance, spacy_opts, punctuation, max_workers, workers_timeout)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m language_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meither `language`, `language_model` or `spacy_instance` must be given\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m language_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m load_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectors\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m load_features \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectors\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m add_features):\n",
      "\u001b[0;31mValueError\u001b[0m: either `language`, `language_model` or `spacy_instance` must be given"
     ]
    }
   ],
   "source": [
    "test_corpus = [ # Feel free to edit this corpus for further testing\n",
    "                # to be sure that your functions meet specifications.\n",
    "    \"The 3 cats sat on the mats!\",\n",
    "    \"1 fish 2 fish Red fish Blue fish\"\n",
    "    \"She sells $ea$shells\"\n",
    "]\n",
    "preproc = preprocess(test_corpus)\n",
    "pp.pprint(preproc.get_tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DuhlHcwQjs8T"
   },
   "outputs": [],
   "source": [
    "dtms = {\n",
    "    \"test_corpus\": preproc.dtm\n",
    "}\n",
    "lda_params = {\n",
    "    'n_topics': 2,\n",
    "    'eta': .01,\n",
    "    'n_iter': 10,\n",
    "    'random_state': 1234,  # to make results reproducible\n",
    "    'alpha': 1/16\n",
    "}\n",
    "\n",
    "models = compute_models_parallel(dtms, constant_parameters=lda_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvB8TL-muaM0"
   },
   "outputs": [],
   "source": [
    "model = models[\"test_corpus\"][0][1]\n",
    "print_ldamodel_topic_words(model.topic_word_, preproc.vocabulary, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_f8YqCs3kVFQ"
   },
   "source": [
    "### Assignment submission\n",
    "\n",
    "After completing the preprocess implementation, download your notebook as a .py file (File > Download > Download .py) and submit the downloaded file for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-RmT0BazGQg"
   },
   "source": [
    "## Topic modeling Amazon Reviews\n",
    "\n",
    "Once you have completed the assignment above, you will be well prepared to start your final project for this unit. The project will include loading Amazon reviews into a corpus for topic modeling. The code below demonstrates topic modeling the reviews for a given brand. Note that the final project will require additional segmentation of the data, which is not done for you in the example here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpcCkHX1P1d6"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "asins = []\n",
    "\n",
    "# To run this code, you will need to download the metadata file from the course\n",
    "# assets and upload it to your Google Drive. See the notes about that file\n",
    "# regarding how it was processed from the original file into json-l format.\n",
    "\n",
    "with gzip.open(\"drive/MyDrive/meta_Clothing_Shoes_and_Jewelry.jsonl.gz\") as products:\n",
    "    for product in products:\n",
    "        data = json.loads(product)\n",
    "        categories = [c.lower() for c in\n",
    "                      list(itertools.chain(*data.get(\"categories\", [])))]\n",
    "        if \"nike\" in categories:\n",
    "            asins.append(data[\"asin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KysOEFGwVfIh"
   },
   "source": [
    "Inspect the first fews ASINs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xycViEOjR4Gr"
   },
   "outputs": [],
   "source": [
    "asins[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0Gska4bVj3b"
   },
   "source": [
    "Check the length, i.e. the number of resulting ASINs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIqkMoAvU-Sz"
   },
   "outputs": [],
   "source": [
    "len(asins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0bwBkHsVrsy"
   },
   "source": [
    "Build a corpus of review texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6HT0Yjy3gNG"
   },
   "outputs": [],
   "source": [
    "review_corpus = []\n",
    "with gzip.open(\"drive/MyDrive/reviews_Clothing_Shoes_and_Jewelry_5.json.gz\") as reviews:\n",
    "    for review in reviews:\n",
    "        data = json.loads(review)\n",
    "        if data[\"asin\"] in asins:\n",
    "            text = data[\"reviewText\"]\n",
    "            review_corpus.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKrH89AKVwCe"
   },
   "source": [
    "Inspect a few of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9Ct4cj7Tih9"
   },
   "outputs": [],
   "source": [
    "for i, review in enumerate(review_corpus[:5]):\n",
    "    print(i, review[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9ppd90QVzgj"
   },
   "source": [
    "Build a TMPreproc object from the review corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taIJ_BZU7E81"
   },
   "outputs": [],
   "source": [
    "pre = preprocess(review_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wq_-wpEE8cnV"
   },
   "outputs": [],
   "source": [
    "dtms = {\n",
    "    \"reviews_corpus\": pre.dtm\n",
    "}\n",
    "lda_params = {\n",
    "    'n_topics': 10,\n",
    "    'eta': .01,\n",
    "    'n_iter': 10,\n",
    "    'random_state': 1234,  # to make results reproducible\n",
    "    'alpha': 1/16\n",
    "}\n",
    "\n",
    "models = compute_models_parallel(dtms, constant_parameters=lda_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGRrYqRuV7nD"
   },
   "source": [
    "Print the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8GYqMPHjVH7"
   },
   "outputs": [],
   "source": [
    "model = models[\"reviews_corpus\"][0][1]\n",
    "print_ldamodel_topic_words(model.topic_word_, pre.vocabulary, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zx2YFruQjWfP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7Mci4DlcB64"
   },
   "source": [
    "## Save your topic model and corpus for use in Lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk6JySzqcGIU"
   },
   "source": [
    "Once you have completed the above assignment, run the following code to save your topic model and your corpus to your Google Drive. You will load this model and use it for document classification in Lab 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVXbd0AUcJIr"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tmtoolkit.topicmod.model_io import save_ldamodel_to_pickle\n",
    "\n",
    "with open(\"drive/MyDrive/MSDS_HW2_model.p\", \"wb\") as modelfile:\n",
    "    save_ldamodel_to_pickle(modelfile, model, pre.vocabulary, pre.doc_labels, dtm=pre.dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVhycwCNcL4w"
   },
   "outputs": [],
   "source": [
    "with open(\"drive/MyDrive/MSDS_HW2_corpus.p\", \"wb\") as corpusfile:\n",
    "    pickle.dump(review_corpus, corpusfile)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of MSDSTopicModel_HW2_BuildATopicModel.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
