{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWgYXQW1kuSg"
   },
   "source": [
    "# MSDS Marketing Text Analytics, Unit 2, Assignment 2: Build a topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLHoXywD4Cqg"
   },
   "source": [
    "## ⚡️ Make a Copy\n",
    "\n",
    "Save a copy of this notebook in your Google Drive before continuing. Be sure to edit your own copy, not the original notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3YbHDKkk1vq"
   },
   "source": [
    "In this assignment, you will implement a topic model preprocessor which can then be applied to the task of topic-modeling Amazon text reviews. Please review the course lectures and documentation up to this point before continuing. Be sure also to be familiar with the [documentation for TMToolkit](https://tmtoolkit.readthedocs.io/en/latest/topic_modeling.html)\n",
    "\n",
    "Be sure to make a copy into your own Drive account before editing this notebook.\n",
    "\n",
    "You will implement a preprocessing function to prepare your corpus for topic modeling. It is recommended that you use a small test corpus (an example is provided below) for development, rather than starting with the full review set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Rfw11JBiZHi"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jpxYdJGQiWqj",
    "outputId": "7bef0172-d4c2-4d9c-e3f3-7d2f899fb1bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lda\n",
      "  Downloading lda-2.0.0.tar.gz (320 kB)\n",
      "\u001b[K     |████████████████████████████████| 320 kB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pbr<4,>=0.6\n",
      "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 18.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.13.0 in /opt/anaconda3/lib/python3.9/site-packages (from lda) (1.21.5)\n",
      "Building wheels for collected packages: lda\n",
      "  Building wheel for lda (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lda: filename=lda-2.0.0-cp39-cp39-macosx_10_9_x86_64.whl size=338490 sha256=d4745c0457340447c70aafb654a67cfcef5b98934a08ac494e017cedf74fd14b\n",
      "  Stored in directory: /Users/donavin/Library/Caches/pip/wheels/3d/b5/8f/1c1c6a2986ad87a19ddd0a0fbe676d6e4764b5c906702fdd95\n",
      "Successfully built lda\n",
      "Installing collected packages: pbr, lda\n",
      "Successfully installed lda-2.0.0 pbr-3.1.1\n",
      "Collecting tmtoolkit\n",
      "  Downloading tmtoolkit-0.11.2-py3-none-any.whl (7.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.5 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from tmtoolkit) (1.4.2)\n",
      "Requirement already satisfied: openpyxl>=3.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from tmtoolkit) (3.0.9)\n",
      "Collecting numpy>=1.22.0\n",
      "  Downloading numpy-1.23.0-cp39-cp39-macosx_10_9_x86_64.whl (18.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 18.1 MB 27.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.5.0 in /opt/anaconda3/lib/python3.9/site-packages (from tmtoolkit) (3.5.1)\n",
      "Requirement already satisfied: xlrd>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from tmtoolkit) (2.0.1)\n",
      "Collecting globre>=0.1.5\n",
      "  Downloading globre-0.1.5.tar.gz (20 kB)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/anaconda3/lib/python3.9/site-packages (from tmtoolkit) (1.7.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.5.0->tmtoolkit) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.5.0->tmtoolkit) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.5.0->tmtoolkit) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.5.0->tmtoolkit) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.5.0->tmtoolkit) (9.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.5.0->tmtoolkit) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.5.0->tmtoolkit) (3.0.4)\n",
      "Requirement already satisfied: et-xmlfile in /opt/anaconda3/lib/python3.9/site-packages (from openpyxl>=3.0.0->tmtoolkit) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.4.0->tmtoolkit) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->tmtoolkit) (1.16.0)\n",
      "Collecting numpy>=1.22.0\n",
      "  Using cached numpy-1.22.4-cp39-cp39-macosx_10_15_x86_64.whl (17.7 MB)\n",
      "Building wheels for collected packages: globre\n",
      "  Building wheel for globre (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for globre: filename=globre-0.1.5-py3-none-any.whl size=19546 sha256=0ba520d8b485b72dbc7d1a3e5d5ac21566ee5c29d8e3d349c06137e57840b4e3\n",
      "  Stored in directory: /Users/donavin/Library/Caches/pip/wheels/93/79/af/43a147d2f2ef8a8a74cf9b72e1947f75b7f960baa3401b0693\n",
      "Successfully built globre\n",
      "Installing collected packages: numpy, globre, tmtoolkit\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.5\n",
      "    Uninstalling numpy-1.21.5:\n",
      "      Successfully uninstalled numpy-1.21.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.22.4 which is incompatible.\u001b[0m\n",
      "Successfully installed globre-0.1.5 numpy-1.22.4 tmtoolkit-0.11.2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "the required package \"spacy\" for text processing is not installed; did you install tmtoolkit with \"recommended\" or \"textproc\" option? see https://tmtoolkit.readthedocs.io/en/latest/install.html for further information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtmtoolkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Corpus\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtmtoolkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TMPreproc\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tmtoolkit'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install lda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install tmtoolkit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtmtoolkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Corpus\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtmtoolkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TMPreproc\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtmtoolkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtopicmod\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_io\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_ldamodel_topic_words\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tmtoolkit/corpus/__init__.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pkg \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbidict\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloky\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m find_spec(pkg) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe required package \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for text processing is not installed; did you install \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m                            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmtoolkit with \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecommended\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtextproc\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m option? see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m                            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://tmtoolkit.readthedocs.io/en/latest/install.html for further information\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenseq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m strip_tags, numbertoken_to_magnitude, simplify_unicode_chars\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_LANGUAGE_MODELS, LANGUAGE_LABELS, simplified_pos\n",
      "\u001b[0;31mRuntimeError\u001b[0m: the required package \"spacy\" for text processing is not installed; did you install tmtoolkit with \"recommended\" or \"textproc\" option? see https://tmtoolkit.readthedocs.io/en/latest/install.html for further information"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tmtoolkit.corpus import Corpus\n",
    "    from tmtoolkit.preprocess import TMPreproc\n",
    "    from tmtoolkit.topicmod.model_io import print_ldamodel_topic_words\n",
    "    from tmtoolkit.topicmod.tm_lda import compute_models_parallel\n",
    "except ModuleNotFoundError:\n",
    "    !pip install lda\n",
    "    !pip install tmtoolkit\n",
    "    from tmtoolkit.corpus import Corpus\n",
    "    from tmtoolkit.preprocess import TMPreproc\n",
    "    from tmtoolkit.topicmod.model_io import print_ldamodel_topic_words\n",
    "    from tmtoolkit.topicmod.tm_lda import compute_models_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: tmtoolkit[textproc_extra]\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tmtoolkit[textproc_extra]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDKzLINMjD2n"
   },
   "source": [
    "**NOTE:** Loading a corpus as a list of strings is not the only way to use tmtoolkit. Given, for example, a large corpus that might not fit in memory, the current approach would not work well. See the tmtoolkit docs on [working with text corpora](https://tmtoolkit.readthedocs.io/en/latest/text_corpora.html) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8g9kSJwi7M7"
   },
   "source": [
    "## Implement a pre-processor\n",
    "\n",
    "Here you will implement a function called `preprocess` which returns the TMPreproc object to be used for topic modeling.\n",
    "\n",
    "The preprocess function will take a list of texts and return a pre-processed corpus object, i.e. a TMPreproc object. Preprocessing should include the following actions on the corpus using the appropriate methods in the TMPreproc class:\n",
    "\n",
    " - lemmatize the texts\n",
    " - convert tokens to lowercase\n",
    " - remove special characters\n",
    " - clean tokens to remove numbers and any tokens shorter than 3 characters\n",
    "\n",
    "The first part of the function to create the corpus and preprocess object are done for you. Your job is to call the specific preprocess functions and to return the resulting preprocess object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQFevchKhohK"
   },
   "outputs": [],
   "source": [
    "def preprocess(texts, lang=\"en\"):\n",
    "    \"\"\"Preprocessor which returns a TMPreproc object processed on corpus as language\n",
    "    specified by lang (defaults to \"en\"):\n",
    "\n",
    "    Should perform all of the following pre-processing functions:\n",
    "     - lemmatize\n",
    "     - tokens_to_lowercase\n",
    "     - remove_special_chars_in_tokens\n",
    "     - clean_tokens (remove numbers, and remove tokens shorter than 2)\n",
    "    \"\"\"\n",
    "    # Here, we just use the index of the text as the label for the corpus item\n",
    "    corpus = Corpus({ i:r for i, r in enumerate(texts) })\n",
    "    preproc = TMPreproc(corpus, language=lang)\n",
    "\n",
    "    # TODO: Complete the implementation of this function and submit the\n",
    "    # .py download of this notebook as your assignment submission.\n",
    "    # lemmatize not working correctly\n",
    "\n",
    "    return preproc.lemmatize().tokens_to_lowercase().remove_special_chars_in_tokens().clean_tokens(remove_shorter_than=2, remove_numbers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4W99eaBwldp3"
   },
   "outputs": [],
   "source": [
    "# %pip install -Iv spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DyxZzrM6aakv",
    "outputId": "fe237b59-35dd-4706-f0f0-743739a4a969"
   },
   "outputs": [],
   "source": [
    "help(TMPreproc.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_jJg5-xRczD"
   },
   "outputs": [],
   "source": [
    "#~~ /autograde # do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2aoBerYnF2M"
   },
   "source": [
    "---\n",
    "### ⚠️  **Caution:** No arbitrary code above this line\n",
    "\n",
    "The only code written above should be the implementation of your graded function. For experimentation and testing, only add code below.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBlUJqEan3oc"
   },
   "source": [
    "## Function development\n",
    "\n",
    "Use this section of code to verify your function implementation. You may change the test_corpus as needed to verify your implementation. The grader will be checking that your function returns a TMPreproc object that meets all of the following critera:\n",
    "\n",
    " - tokens are lemmatized\n",
    " - tokens are converted to lowercase\n",
    " - special characters are removed from tokens\n",
    " - tokens shorter than 3 characters and numerics are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBlpS0RsrJ3s"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "_tk2oDy5j1gQ",
    "outputId": "2ad74972-4818-41db-d202-fe2bb860dea3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (3.3.0) requires spaCy v3.3 and is incompatible with the current spaCy version (2.3.7). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7ef4ed799e41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"She sells $ea$shells\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpreproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-919688a0195e>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(texts, lang)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Here, we just use the index of the text as the label for the corpus item\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mpreproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTMPreproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# TODO: Complete the implementation of this function and submit the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tmtoolkit/preprocess/_tmpreproc.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, docs, language, language_model, n_max_processes, stopwords, special_chars, enable_vectors, spacy_opts, loading_from_state)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# setting this unfortunately increases the reference count for a TMPreproc object and hence makes it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tmtoolkit/preprocess/_tmpreproc.py\u001b[0m in \u001b[0;36m_setup_workers\u001b[0;34m(self, docs, initial_states, docs_are_tokenized)\u001b[0m\n\u001b[1;32m   1845\u001b[0m             logger.debug('loading spaCy language model %s with options: %s'\n\u001b[1;32m   1846\u001b[0m                          % (self.language_model, str(spacy_opts)))\n\u001b[0;32m-> 1847\u001b[0;31m             \u001b[0mnlp_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mspacy_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m             \u001b[0;31m# distribute work evenly across the worker processes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# installed as package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# path to model data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;34m\"\"\"Load a model from an installed package.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/en_core_web_sm/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, **overrides)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath2str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, **overrides)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mfactory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m             \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mcreate_pipe\u001b[0;34m(self, name, config)\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE108\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE002\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mfactory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[E002] Can't find factory for 'tok2vec'. This usually happens when spaCy calls `nlp.create_pipe` with a component name that's not built in - for example, when constructing the pipeline from a model's meta.json. If you're using a custom component, you can write to `Language.factories['tok2vec']` or remove it from the model meta and add it via `nlp.add_pipe` instead.\""
     ]
    }
   ],
   "source": [
    "test_corpus = [ # Feel free to edit this corpus for further testing\n",
    "                # to be sure that your functions meet specifications.\n",
    "    \"The 3 cats sat on the mats!\",\n",
    "    \"1 fish 2 fish Red fish Blue fish\"\n",
    "    \"She sells $ea$shells\"\n",
    "]\n",
    "preproc = preprocess(test_corpus)\n",
    "pp.pprint(preproc.get_tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DuhlHcwQjs8T"
   },
   "outputs": [],
   "source": [
    "dtms = {\n",
    "    \"test_corpus\": preproc.dtm\n",
    "}\n",
    "lda_params = {\n",
    "    'n_topics': 2,\n",
    "    'eta': .01,\n",
    "    'n_iter': 10,\n",
    "    'random_state': 1234,  # to make results reproducible\n",
    "    'alpha': 1/16\n",
    "}\n",
    "\n",
    "models = compute_models_parallel(dtms, constant_parameters=lda_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvB8TL-muaM0"
   },
   "outputs": [],
   "source": [
    "model = models[\"test_corpus\"][0][1]\n",
    "print_ldamodel_topic_words(model.topic_word_, preproc.vocabulary, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_f8YqCs3kVFQ"
   },
   "source": [
    "### Assignment submission\n",
    "\n",
    "After completing the preprocess implementation, download your notebook as a .py file (File > Download > Download .py) and submit the downloaded file for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-RmT0BazGQg"
   },
   "source": [
    "## Topic modeling Amazon Reviews\n",
    "\n",
    "Once you have completed the assignment above, you will be well prepared to start your final project for this unit. The project will include loading Amazon reviews into a corpus for topic modeling. The code below demonstrates topic modeling the reviews for a given brand. Note that the final project will require additional segmentation of the data, which is not done for you in the example here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpcCkHX1P1d6"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "asins = []\n",
    "\n",
    "# To run this code, you will need to download the metadata file from the course\n",
    "# assets and upload it to your Google Drive. See the notes about that file\n",
    "# regarding how it was processed from the original file into json-l format.\n",
    "\n",
    "with gzip.open(\"drive/MyDrive/meta_Clothing_Shoes_and_Jewelry.jsonl.gz\") as products:\n",
    "    for product in products:\n",
    "        data = json.loads(product)\n",
    "        categories = [c.lower() for c in\n",
    "                      list(itertools.chain(*data.get(\"categories\", [])))]\n",
    "        if \"nike\" in categories:\n",
    "            asins.append(data[\"asin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KysOEFGwVfIh"
   },
   "source": [
    "Inspect the first fews ASINs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xycViEOjR4Gr"
   },
   "outputs": [],
   "source": [
    "asins[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0Gska4bVj3b"
   },
   "source": [
    "Check the length, i.e. the number of resulting ASINs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIqkMoAvU-Sz"
   },
   "outputs": [],
   "source": [
    "len(asins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0bwBkHsVrsy"
   },
   "source": [
    "Build a corpus of review texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6HT0Yjy3gNG"
   },
   "outputs": [],
   "source": [
    "review_corpus = []\n",
    "with gzip.open(\"drive/MyDrive/reviews_Clothing_Shoes_and_Jewelry_5.json.gz\") as reviews:\n",
    "    for review in reviews:\n",
    "        data = json.loads(review)\n",
    "        if data[\"asin\"] in asins:\n",
    "            text = data[\"reviewText\"]\n",
    "            review_corpus.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKrH89AKVwCe"
   },
   "source": [
    "Inspect a few of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9Ct4cj7Tih9"
   },
   "outputs": [],
   "source": [
    "for i, review in enumerate(review_corpus[:5]):\n",
    "    print(i, review[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9ppd90QVzgj"
   },
   "source": [
    "Build a TMPreproc object from the review corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taIJ_BZU7E81"
   },
   "outputs": [],
   "source": [
    "pre = preprocess(review_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wq_-wpEE8cnV"
   },
   "outputs": [],
   "source": [
    "dtms = {\n",
    "    \"reviews_corpus\": pre.dtm\n",
    "}\n",
    "lda_params = {\n",
    "    'n_topics': 10,\n",
    "    'eta': .01,\n",
    "    'n_iter': 10,\n",
    "    'random_state': 1234,  # to make results reproducible\n",
    "    'alpha': 1/16\n",
    "}\n",
    "\n",
    "models = compute_models_parallel(dtms, constant_parameters=lda_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGRrYqRuV7nD"
   },
   "source": [
    "Print the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8GYqMPHjVH7"
   },
   "outputs": [],
   "source": [
    "model = models[\"reviews_corpus\"][0][1]\n",
    "print_ldamodel_topic_words(model.topic_word_, pre.vocabulary, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zx2YFruQjWfP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7Mci4DlcB64"
   },
   "source": [
    "## Save your topic model and corpus for use in Lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk6JySzqcGIU"
   },
   "source": [
    "Once you have completed the above assignment, run the following code to save your topic model and your corpus to your Google Drive. You will load this model and use it for document classification in Lab 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVXbd0AUcJIr"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tmtoolkit.topicmod.model_io import save_ldamodel_to_pickle\n",
    "\n",
    "with open(\"drive/MyDrive/MSDS_HW2_model.p\", \"wb\") as modelfile:\n",
    "    save_ldamodel_to_pickle(modelfile, model, pre.vocabulary, pre.doc_labels, dtm=pre.dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVhycwCNcL4w"
   },
   "outputs": [],
   "source": [
    "with open(\"drive/MyDrive/MSDS_HW2_corpus.p\", \"wb\") as corpusfile:\n",
    "    pickle.dump(review_corpus, corpusfile)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of MSDSTopicModel_HW2_BuildATopicModel.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
