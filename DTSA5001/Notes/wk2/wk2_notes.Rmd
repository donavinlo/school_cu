---
title: "Week 2 Notes"
output:
  pdf_document: default
  html_notebook: default
---
 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
\begin{center}\textbf{\Huge Reading Notes} \end{center}

\begin{center}\textbf{\Large Conditional Probability} \end{center}
\textbf{Conditional Probability: }the probability of A given that B occurred. This can be represented with:
$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$
Easiest way to remember formula is by remembering that given a conditional statement, B has already occurred. In other words, we are reducing the sample space to B only. Therefore, P(B) is on the bottom. Since we want to know the probability of A given that B occurred, the numerator needs to represent when both A and B occurs. That's represented by the intersection.

# Law of Total Probability
For two events B and A, we can find the total probability of A using the following formula:
$$P(A) = P(A|B)P(B) + P(A|B^{c})P(B^{c})$$
Basically, this is the weighted average of of when A is in that space or not$(P(B) and P(B^{c}))$. I.e. when B occurs and when it doesn't. We can also rewrite this as:
$$P(B) = \sum_{k=1}^{n}P(B|A_{k})P(A_{k})$$
We need to keep in mind that all A_{i}'s are mutually exclusive.
# Bayes' Rule
Bayes' Rule provides a useful way of going between P(A|B) and P(B|A). The formula is
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)} = \frac{P(B|A)P(A)}{P(A|B)P(B) + P(A|B^{c})P(B^{c})}$$
The very right hand side is often referred to as the \textit{Law of Total Probability} (LOTP)

### Bayes Rule Example
Lets consider an example. Frodo needs to return a piece of jewelry to the store (Mordor Arts & Crafts, Inc.). His friend, Sam, has a car, and if Sam goes with Frodo, there is a .9 probability that Frodo gets the jewelry to the store. However, if Sam doesn’t go with Frodo (and Frodo must get there by himself), he only has a .1 probability of making it to the store. Sam is a good friend, and there is a .8 probability that he goes with Frodo. Conditioned on the fact that Frodo successfully returned the jewelry to Mordor, what is the probability that Sam went with him?  
This is a classic example of Bayes’ Rule. Let  $F$ be the event that Frodo gets the jewelry to the store, and  $S$ be the event that Sam goes with Frodo to the store. We are interested in  $P(S|F)$, which, using the definition of Bayes’ Rule, we can write as: $$P(S|F) = \frac{P(F|S)P(S)}{P(F)}$$
We are given $P(F|S) = 0.9$ (if Sam comes, Frodo has a.9 probability of making it) and P(S) = 0.8(Sam has a .8 probability of coming). However, we need to find $P(F)$ using the LOTP.
$$P(S|F) = \frac{P(F|S)P(S)}{P(F|S)P(S) + P(F|S^{c})P(S^{c})}$$

We know $P(F|S^{c})$ = .1 (if Sam doesn't go, Frodo only has a 0.1 probability of making it) and $P(S^{c})$ = 0.2 (Same has a 0.8 probability of going, so he has a 0.2 probability of not going). This information gives us:
$$\frac{0.9 \cdot 0.8}{0.9 \cdot 0.8 + 0.1 \cdot 0.2} = 0.97$$
This high probability makes since Frodo has a very good chance of making it if Sam is with him and Sam had a high probability of coming. 

# Inclusion/Exclusion
Useful was to find the probability of the union of multiple events:
$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$
Exending this probability to more than two sets, we use th form:
$$P(Union of Many Events) = P(Singles) - P(Doubles) + P(Triples) - P(Quadruples) ...$$
We do thia to account for over counting. For example, adding P(A) and P(B) would count P(A$\cap$B) twice; thefore, we subtract one of them out. 

# Independence
\textbf{Independence: }Two events are independent if knowing the outcome of one event does not affect the probability of the other event occurring. Or, in mathematical terms:
$$P(A|B)=P(A),\space P(B|A)=P(B)$$

If events A and B are independent, we get the following:
$$P(A \cap B) = P(A)P(B)$$
Remember that we have to prove the two are independent before doing this. Also, independence isn't the same as disjoint $P(A\cap B)=0$. This actually means the two are dependent: if one event occurs than the other one can't since there is no intersection. 

### Example
An experiment where we flip a coin and roll a die to get even or odd. The coin flip is independent from either die result. However, The even or odd roll is dependent and disjoint because you can't roll both an even and an odd at the same time. 

# Conditional Independence
Events A and B are conditionally independent given C if:
$$P(A \cap B|C) = P(A|C)P(B|C)$$
Remember: conditional independence does not imply regular independence

### Example
You roll two fair die. Intuitively, the results of the two are independent. Knowing the one die is 6 doesn't change the outcome of the other. If we put a condition that the total between the two is 7, then the two die rolls are no longer independent. If I state the first die is 4, then you know the second die is 3. In this case, the two are marginally independent but conditionally dependent.

\begin{center}\textbf{\Large The Birthday Problem} \end{center}

\textbf{Problem: }Find a match. In other words, a day where there are two people with the same birthday within a group of \textit{n} people. We don't care about the year; therefore, each day has a 1/365 chance of being chosen.  
  
It will be easier to find the complement. I.e. if no one shared the same birthday. Because we choose one at a time. let $A_{i}$ represent the $i^{th}$ person to choose a birthday. Thus, the probability when the second person is choosing a birthday it can be represented by $P(A_{2}|A_{1}) = 364/365$.

\begin{center}\textbf{\Large Monty Hall} \end{center}
### Problem Statement
You're on a game show and there is a car behind one of three closed doors; there are goats behind the other two. You pick a door, and the host, Monty, opens one of the doors that he is sure the car is not behind. Now you have two choices. Do you keep the door you chose or do you switch to door three? The answer is that of the two doors left, the one that you didn't choose has a 2/3 chance of having the car. Therefore, you should switch.  
Let see C represent the door with the car (we'll just say door 1) and let G represent the Goat door Monty reveals. We want to know P(C|G), the probability the car is behind door 1 given Monty opened Door 2. By Bayes Rule:
$$P(C|G) = \frac{P(G|C)P(C)}{P(G)}$$
What we know is P(C) is 1/3. P(G) is 1/2 since Monty choose 1 of 2 doors to open. P(G|C) (the probability Monty opens Door 2 given that the car is behind Door 1). If the car is behind door 1, that mens there are goats behind 2 and 3. Thus, Monty would have a choice of which door to choose.: $P(C|G) = \frac{1}{2}$. All together we get:
$$P(C|G) = \frac{1/2 \cdot /1/3}{1/2} = \frac{1}{3}$$
So, the probability the car is behind door 1 is 1/3.

\begin{center}\textbf{\Large Intro to Random Variables} \end{center}

Can think of a random variable as some sort of Machine that randomly spits our numbers. Unlike a function, which is very deterministic, the output of a random variable is uncertain.
Usually referenced with large letters, such as X.

\begin{center}\textbf{\Large Properties of Random Variables} \end{center}

## Distribution
The 'type' of random variable. A distribution describes the pattern that the random variable follows. Example, we have a recipe (distribution) that gives instructions to make a specific 'meal' (the random variable)  
  
Each distribution has its own properties (expectation, variance, pmf, etc.)

### Expectation
Another name for its average., denoted by E(X)

### Variance
How much spread is inherent in a certain random variable, notated by $Var(X)$

### PMFs and CDFs
Random Variables have probability mass functions (PMF) for discrete random variables and probability density functions (PDF) for continuous variables. PMF gives the probability that the random variable takes on a certain value. Denoted by P(X = x).  
  
\textbf{cumulative density functions} is a step further than PMF. Gives the probability $P(X \leq x)$. Notation for CDF is $F(x)$ where X is the random variable. 

### Support
The set of possible values that the random variable can take on.

### Calculating Expectation and Variance
$$E(X) = \sum_{i}x_{i}P(X = x_{i})$$
$$Var(X) = \sum_{i}(x_{i} - E(X))^{2}P(X = x_{i})$$
Standard deviation is the square root of the variance. 

\begin{center}\textbf{\Large Binomial}\end{center}

The binomial distribution is a discrete distribution. In other words, the support is discrete. We also state that each distribution has a \textbf{story}.

\textbf{Binomial Distribution Story: }we perform $n$ independent trials, each with only two outcomes (usually we think of these two outcomes as success or failure) and with a probability of success p that stays constant from trial to trial. The notation for this is:  
$$X \sim Bin(n,p)$$
The $\sim$ means 'has the distribution of...'
\textbf{Example: }Flipping a coin a number of $n$ times in the hopes of getting heads each time.

\textbf{Parameters: }The  n is the number of trials and the p is the probability success of each trial

\textbf{Characterstics/Properties}:
$$E(X) = np,\space Var(X) = npq$$  
Q is 1-p. Makes sense the variance grows as $n$ grows: the more trials means more variance and is maximized at p=1/2. Again, makes sense since if one has a higher probability, the less uncertainty of a number appearing.  
\textbf{PMF: }
$$P(X = x) = {n \choose x}p^{x}q^{n-x}$$
The reason we have ${n \choose x}$ so we can count out all the permutations that represent the same combination. We don't care about the order we get the number of successes. Thus, ${n \choose x}$ gives us all of our desirable permutations. Thus, we multiply the probability of each desirable permutations by the total number of desirable permutations to get the overall probability that we find a desirable permutation. In other words, \textbf{ the $p^{x}q^{n-x}$ part represents the probability and the ${n \choose x}$ is the number of desirable permutations}. The \textbf{support} of a binomial distribution is the integers 0 to n.

### binomial in R

```{r dbinom}
#Find P(X=3) where X ~ Bin(10, 0.5)
dbinom(3,10, 0.5)
```

```{r pbinom}
#Find P(X<=6) where X ~ Bin(15, 1/3)
dbinom(6,15, 1/3)
```

```{r qbinom}
#Find the value of x such that P(X <= x) = .9, where X ~ Bin(50, 1/5)
qbinom(0.9,50, 1/5)
```

```{r rbinom}
#generate 5 random draws from X, where X ~ Bin(30, 1/4)
rbinom(5, 30, 1/4)
```

\pagebreak

\begin{center}\textbf{\Huge Lecture Notes} \end{center}

# Law of Total Probability Example
Suppose your company has developed a new terst for a disease. Let event A be the event that a randomly selected individual has the disease and, from other data, you know that 1 in 1000 people has the disease. Thus, P(A) = .001. Let B be the event that a positive test result is received for the randomly selected individual. Your company collects data on their new test and find the following:  
  
\textbullet $P(B|A)$ = 0.99  (P(+ test result| has disease))  
\textbullet $P(B^{c}|A)$ = 0.01  (P(- test result| has disease))  
\textbullet $P(B|A^{c})$ = 0.02  (P(+ test result| no disease))  
  
Calculate the probability that the person has the disease, given a positive test result. That is, find P(A|B).

$$P(A|B) = \frac{P(A \cap B)}{P(B)} = \notag\\ \frac{P(B|A)P(A)}{P(B)} \notag\\ = \frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|A^{c})P(A^{c})} \notag\\= \frac{(0.99)(0.001)}{(0.99)(0.001) + (0.02)(0.999)} = .0472$$

P(A) = 0.001 is referred to as our \textbf{prior prob of A}  
P(A|B) = 0.04 is referred to as our \textbf{posterior prob of A}

# Independence
Events $A_{1}...A_{n}$ are \textbf{mutually independent} if for every k(k=2,3,...n) and every subset of indices $i_{1}, i_{2}, ..., i_{k}$:
$$P(A_{i_{1}} \cap A_{i_{2}} \cap ... \cap A_{i_{n}}) = P(A_{i_{1}})P(A_{i_{2}})...P(A_{i_{k}})$$ 

### Side note

\textbf{two mutually exclusive events are not independent. If one event occurs, then the other one cannot occur. }