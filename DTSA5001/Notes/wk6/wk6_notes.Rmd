---
title: "Week 6 Notes: Central Limit Theorem and Conditional Expectation"
author: "D. ODay"
date: "10/28/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\begin{center}\textbf{\Huge Reading Notes} \end{center}

# Law of Large Numbers
We define the sample mean as the following:
$$\bar{X_{n}} = \frac{X_{1} +...+ X_{n}}{n}$$

### Strong Law of Large Numbers
The sample mean $\bar{X_{n}}$ goes to the true mean $\mu$ as $n \rightarrow \infty$
with probability 1.

### Weak Law of Large Numbers
Basically, the larger the sample size grows the smaller the error of the sample mean is. In other words, $\bar{X_{n}} - \mu$ becomes closer to 0.  
  
Both of these lawsd say, in summary, the sample mean will converge on the true mean.

# Central Limit Theorem
Sample mean has the following:
$$\bar{X_{n} \rightarrow^{D} N(\mu, \frac{\sigma^{2}}{n})}$$

The Central Limit Theorem states, \textbf{for large $n$, the distribution of the sample mean approaches a Normal Distribution} (As shown in the formula above). $\rightarrow^{D}$ means 'converges in distribution'. The intuition is as $n$ becomes large, the mean will be be unaffected and the variance will shring close to 0; therefore, the distribution will essentially be a constant.

# Conditional Expectation
Knowing the Law ot total Probability, we can get the \textbf{Law of Total Expectation} as the following:
$$E(X) = E(X|A)P(A) + E(X|A^{c})P(A^{c})$$

If we see $E(X|X=x)$ then er know it is asking for the expectation of the random variable $X$, given that we know the random variable $X$ crystallizes to the value $x$. Same idea applies to $E(h(X)|X)$. Which again, this equals $h(X)$ (let $h(X)$ be a funcation). All we do is plug in X into $h(X)$. 

## Conditional Expectation for Different Random Variables
we have $E(Y|X)$ which means that we want the best prediction for Y given that we know the random variable X. In other words, does Y have a different distribution if we know the value of X?

### Example
Let's say we have $Y \sim Bin(X, 0.5)$ and $X \sim DUnif(1,10)$. Dunif is discrete uniform. Just means we can only have integer values, not continuous. Can think of the overall structure as flipping a coin and counting heads.For $E(Y|X)$, we know that $Y$ is Binomial; therefore, it's expectation is $np$. For our case, we know p =$\frac{1}{2}$. Since $n$ is X in this distribution, we know the number of trials is random; however, since we're conditioning on X, we can say we \textit{know} the value. Thus, since $X$ is known, we can say $E(Y|X) = \frac{X}{2}$. \textbf{If X and Y are independent}, then then $E(X|Y) = E(Y)$. That's because knowing the random variable $X$ does not give any information that is helpful in predicting Y. 

\pagebreak

\begin{center}\textbf{\Huge Lecture Notes} \end{center}

# Introduction to the Central Limit Theorem

The sample mean is also know as the \textit{estimator} of$\mu$. It has a distribution of it's own, which is referred to as the \textbf{sampling distribution of the sample mean}. The sampling distribution depends on the following: 
\begin{itemize}
\item The sample size n
\item The population distribution of the $X_{i}$
\item The method of sampling
\end{itemize}

For the Central Limit Theorem, the typical rule of thumb is $n \geq 30$. Think of using a t-test vs a z-test.  
  
\textbf{Proposition: }If $X_{1}, X_{2},...,X_{n}$ are iid with $X_{i} \sim N(\mu, \sigma^{2})$ then $\bar{X} \sim N(\mu, \sigma^{2}/n)$  
  
\textbf{Proposition: }If $X_{1}, X_{2},...,X_{n}$ are independent with $X_{i} \sim N(\mu_{i}, \sigma^{2}_{i})$ then $\sum_{i=1}^{n}X_{i} \sim N(\sum_{i=1}^{n}\mu_{i}, \sum_{i=1}^{n}\sigma_{i}^{2})$

### Example 1
Suppose you have 3 errans to do in three different stores. Let $T_{i}$ be the time to make the $i^{th}$ purchase for $i$ = 1,2,3. Let $T_{4}$ be the total walking time between stores. Suppose $T_{1} \sim N(15,16)$, $T_{2} \sim N(5,1)$, 
$T_{3} \sim N(8,4)$ and $T_{4} \sim N(12,9)$. Assume $T_{1}, T_{2}, T_{3}, T{4}$ are independent. If you leave at 10 a.m. and you want to tell a colleague, "I'll be back by time t", what should t be so that you will return by that time with probability 0.99?  
  
Let $T_{0} = T_{1} + T_{2} + T_{3}+ T_{4}$ = total time  
$E(T_{0}) = 15 + 5 + 8 + 12$ = 40 minutes  
$V(T_{0}) = 16 + 1 + 4 + 9$ = 30 minutes.  
  
Thus, $T_{0} \sim N(40 30)$. We want to find $t$ so that $P(T_{0} \leq t)$ = 0.99.  Now we just solve the problem using the new normal distribution we have ( we know how to solve Normal Random from before.):

$$\begin{aligned}
P(\frac{T_{0}-40}{\sqrt{30}} \leq \frac{t-40}{\sqrt{30}}) = .99 \\
\Rightarrow P(Z \; \leq \frac{t-40}{\sqrt{30}}) = \phi(\frac{t-40}{\sqrt{30}})
=0.99 \\
\Rightarrow Lookup: \phi(2.33) = .99 \Rightarrow \frac{t-40}{\sqrt{30}} = 2.33 \Rightarrow t=52.76 \; minutes
\end{aligned}$$


\pagebreak

\begin{center}\textbf{\Huge Formula Summary} \end{center}

### Sample Mean
$$\bar{X_{n}} = \frac{X_{1} +...+ X_{n}}{n}$$
$$\bar{X_{n} \rightarrow^{D} N(\mu, \frac{\sigma^{2}}{n})}$$
### Sums of Variance of Normal Sample
$$Var(X+Y) = \sqrt{\frac{\sigma_{1}^{2}}{n_1} + \frac{\sigma_{2}^{2}}{n_2}}$$

\textbf{Law of Total Expectation}
$$E(X) = E(X|A)P(A) + E(X|A^{c})P(A^{c})$$

as X and Y are independent

so W = X+Y is also a normal distribution with mean = 0 and variance = 2 sigma²

U= 2X also normal because linear combination of independent variable or scaler multiplication is also a normal distribution

So U is also normal with mean= 0 and variance 4sigma²

But W and U do not have same distribution because variance is different.



