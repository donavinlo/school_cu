---
title: "Week 2 R Code"
output:
  pdf_document: default
  html_notebook: default
---
The data for these courses can be found here:
[Important Github for Specialization Data Examples](https://github.com/bzaharatos/-Statistical-Modeling-for-Data-Science-Applications)



```{r load_packages}
library("ggplot2")
library("RCurl")
library("purrr")
library("tidyr")
```

The following daataset contains measurements related to the  impact of three advertising medias on sales of a product, P. The variables are: 
  
- `Youtube`: the advertising budget allocated to YouTube. Measures in thousands of dollars  
- `facebook`: the advertising budget allocated to Facebook. Measures in thousands of dollars  
-`newspaper`: the advertising budget allocated to a local newspaper. Measures in thousands of dollars  
- `sales`: the values in the $i^{th}$ row of the sales columns is a measurement of the sales (in thousands of units) for product P for company i. 
```{r get_data}
url = getURL(paste0("https://raw.githubusercontent.com/bzaharatos/-Statistical", 
                    "-Modeling-for-Data-Science-Applications/master/Modern%20R",
                    "egression%20Analy", "sis%20/Datasets/marketing.txt"))

marketing = read.csv(text=url, sep= "")
data(marketing)
head(marketing)
```

# Exploratory Data Analysis
Before we model the data, let's first exlpore it. We'll check for any missing values. the look at univariate and bivariate summaries of the data  
  
### Missing Data and Univariate Explorations
Are there any missing values coded as `NA`? Or, are there any odd values for variables, e.g. `9999` or `0` possibly standing in for a missing value?

```{r missing_val}
dim(marketing)
cat("There are", sum(is.na(marketing)), "missing data values.\n")
summary(marketing)

```

As we can see, this dataset has 200 rows and 4 columns. There are 0 missing values. No alarming statistics shown in the summary.  
  
The issue with this dataset is we can't split into test and training datasets. 
```{r univariate_explore1}
marketing %>%
  keep(is.numeric) %>%
  gather %>%
  ggplot(aes(value)) + 
    facet_wrap(~key, scales="free") +
    geom_histogram(bins=12, color="black", fill="#CFB87C", alpha=0.8) +
    theme_bw()
  

```

None of the predictors don't look normally distributed. However, our assumption doesn't require the predictors to be normally distributed. We also notice the newspaper variable potentially has some outliers.   
  
`Sales` sort of has a bell shaped curve. The response variable should be normal, according to our assumption. However, that is a lower-tiered assumption on the list. Our Least squares regression doesn't require normality; however, our inferences do. 
  
As we said earlier, `newspaper` potentiall has some outliers. Let's look at some boxplots to see in further detail.  
R classifies potential outliers by the "IQR criterion". This means observations above \textbf{$q_{0.75}$ + 1.5 X IQR} or below \textbf{$q_{0.25}$ - 1.5 X IQR} are classified as outliers where  
- $q_{0.25}$ is the first quartile  
- $q_{0.75}$ is the third quartile  
- IQR is the interquartile range, defined as the difference between the third and first quartile.  
  
A boxplot will flag the outliers:

```{r outlier_boxplot}
par(mfrow = c(1,4))
boxplot(marketing$sales, main="Sales",col = "#CFB87C")
boxplot(marketing$youtube, main="YouTube",col = "#CFB87C")
boxplot(marketing$facebook, main="Facebook",col = "#CFB87C")
boxplot(marketing$newspaper, main="Newspaper",col = "#CFB87C")
```
```{r outlier_statement}
cat("The outliers for the Newspaper variable are ", 
    boxplot.stats(marketing$newspaper)$out[1], " and ",
    boxplot.stats(marketing$newspaper)$out[2])
```

These two outliers can affect the regression.  
  
### Bivariate Explorations
Let's now explore how the variables may or may not relate to each other. First, calculate the correlations between variables. Correlations can help us measure the strength of the liner relationship between variables. We'll do this with the `corrplot()` function.

```{r corr_matrix}
library(corrplot)
corrplot(cor(marketing), method="ellipse", addCoef.col="black", tl.col="black")

```

We can see sales high high correlation with YouTube, a bit with Facebook and almost none with newspaper. This shows the measure of the strength of the linear relationship.

```{r pair_plots}
#pairs(marketing, main="Marketing Data", pch=21, bg=c("#CFB87C"))
#Will need to double click to see actual relationship better
# Image saved as img in file. R markdown not allowing pair plot.
```
Looking at plot we can see the relationship between the different variables.

## Linear Regression Modeling

### Sums of squares and $R^{2}$ for simple linear regression
First, let's fit the entire dataset. In addition to running a summary of the model, (using `summary`), we'll also run an "analysis of variance", using the `anova()` function. The analysis of variance decomposes the total variability (TSS) into the explained variabiliy (ESS), and the residual/unexplained variability (RSS). It also produces an "F-test that we'll learn how to interpret in the next module. 
```{r first_summary}
lm_marketing = lm(sales ~ facebook, data = marketing)
summary(lm_marketing)
anova(lm_marketing)

```

From the output we see that:  
- ESS = 2590.1
- RSS = 5210.6
- TSS = 2590.1 + 5210.6 = 7800.7

** Rest of code in own notes**