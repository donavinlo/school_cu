{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The F-test in R\n",
    "\n",
    "In this lesson, we will perform both the full and partial F-tests in R.\n",
    "\n",
    "Recall again, the Amazon book data. The data consists of data on $n = 325$ books and includes measurements of:\n",
    "\n",
    "- `aprice`: The price listed on Amazon (dollars)\n",
    "\n",
    "\n",
    "- `lprice`: The book's list price (dollars)\n",
    "\n",
    "\n",
    "- `weight`: The book's weight (ounces)\n",
    "\n",
    "\n",
    "- `pages`: The number of pages in the book\n",
    "\n",
    "\n",
    "- `height`: The book's height (inches)\n",
    "\n",
    "\n",
    "- `width`: The book's width (inches)\n",
    "\n",
    "\n",
    "- `thick`: The thickness of the book (inches)\n",
    "\n",
    "\n",
    "- `cover`: Whether the book is a hard cover of paperback.\n",
    "\n",
    "\n",
    "- And other variables...\n",
    "\n",
    "We'll explore model using `lprice`, `pages`, and `width` to predict `aprice`. But first, we'll repeat the data cleaning from our lesson on t-tests. For all tests in this lesson, let $\\alpha = 0.05$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Title'</li>\n",
       "\t<li>'Author'</li>\n",
       "\t<li>'List.Price'</li>\n",
       "\t<li>'Amazon.Price'</li>\n",
       "\t<li>'Hard..Paper'</li>\n",
       "\t<li>'NumPages'</li>\n",
       "\t<li>'Publisher'</li>\n",
       "\t<li>'Pub.year'</li>\n",
       "\t<li>'ISBN.10'</li>\n",
       "\t<li>'Height'</li>\n",
       "\t<li>'Width'</li>\n",
       "\t<li>'Thick'</li>\n",
       "\t<li>'Weight..oz.'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Title'\n",
       "\\item 'Author'\n",
       "\\item 'List.Price'\n",
       "\\item 'Amazon.Price'\n",
       "\\item 'Hard..Paper'\n",
       "\\item 'NumPages'\n",
       "\\item 'Publisher'\n",
       "\\item 'Pub.year'\n",
       "\\item 'ISBN.10'\n",
       "\\item 'Height'\n",
       "\\item 'Width'\n",
       "\\item 'Thick'\n",
       "\\item 'Weight..oz.'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Title'\n",
       "2. 'Author'\n",
       "3. 'List.Price'\n",
       "4. 'Amazon.Price'\n",
       "5. 'Hard..Paper'\n",
       "6. 'NumPages'\n",
       "7. 'Publisher'\n",
       "8. 'Pub.year'\n",
       "9. 'ISBN.10'\n",
       "10. 'Height'\n",
       "11. 'Width'\n",
       "12. 'Thick'\n",
       "13. 'Weight..oz.'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"Title\"        \"Author\"       \"List.Price\"   \"Amazon.Price\" \"Hard..Paper\" \n",
       " [6] \"NumPages\"     \"Publisher\"    \"Pub.year\"     \"ISBN.10\"      \"Height\"      \n",
       "[11] \"Width\"        \"Thick\"        \"Weight..oz.\" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "     aprice            lprice           pages           width      \n",
       " Min.   :  0.770   Min.   :  1.50   Min.   : 24.0   Min.   :4.100  \n",
       " 1st Qu.:  8.598   1st Qu.: 13.95   1st Qu.:208.0   1st Qu.:5.200  \n",
       " Median : 10.200   Median : 15.00   Median :320.0   Median :5.400  \n",
       " Mean   : 13.010   Mean   : 18.58   Mean   :335.8   Mean   :5.584  \n",
       " 3rd Qu.: 13.033   3rd Qu.: 19.95   3rd Qu.:416.0   3rd Qu.:5.900  \n",
       " Max.   :139.950   Max.   :139.95   Max.   :896.0   Max.   :9.500  \n",
       "     weight          height           thick       cover  \n",
       " Min.   : 1.20   Min.   : 5.100   Min.   :0.100   H: 89  \n",
       " 1st Qu.: 7.80   1st Qu.: 7.900   1st Qu.:0.600   P:235  \n",
       " Median :11.20   Median : 8.100   Median :0.900          \n",
       " Mean   :12.48   Mean   : 8.161   Mean   :0.908          \n",
       " 3rd Qu.:16.00   3rd Qu.: 8.500   3rd Qu.:1.100          \n",
       " Max.   :35.20   Max.   :12.100   Max.   :2.100          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(RCurl) #a package that includes the function getURL(), which allows for reading data from github.\n",
    "library(ggplot2)\n",
    "url = getURL(paste0(\"https://raw.githubusercontent.com/bzaharatos/\",\n",
    "                    \"-Statistical-Modeling-for-Data-Science-Applications/\",\n",
    "                    \"master/Modern%20Regression%20Analysis%20/Datasets/amazon.txt\"))\n",
    "amazon = read.csv(text = url, sep = \"\\t\")\n",
    "names(amazon)\n",
    "df = data.frame(aprice = amazon$Amazon.Price, lprice = as.numeric(amazon$List.Price),  \n",
    "                pages = amazon$NumPages, width = amazon$Width, weight = amazon$Weight..oz,  \n",
    "                height = amazon$Height, thick = amazon$Thick, cover = amazon$Hard..Paper)\n",
    "\n",
    "#cleaning the data, as was done in our lesson on t-tests\n",
    "df$weight[which(is.na(df$weight))] = mean(df$weight, na.rm = TRUE)\n",
    "df$pages[which(is.na(df$pages))] = mean(df$pages, na.rm = TRUE)\n",
    "df$height[which(is.na(df$height))] = mean(df$height, na.rm = TRUE)\n",
    "df$width[which(is.na(df$width))] = mean(df$width, na.rm = TRUE)\n",
    "df$thick[which(is.na(df$thick))] = mean(df$thick, na.rm = TRUE)\n",
    "df = df[-205,]\n",
    "summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the \"full\" model from our lesson on t-tests, namely, the model that includes `lprice`, `pages`, and `width` as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = aprice ~ lprice + pages + width, data = df)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-19.3092  -1.7824  -0.0695   1.3374  22.9248 \n",
       "\n",
       "Coefficients:\n",
       "             Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  0.862994   1.573723   0.548    0.584    \n",
       "lprice       0.854834   0.017848  47.895  < 2e-16 ***\n",
       "pages       -0.006044   0.001348  -4.482 1.03e-05 ***\n",
       "width       -0.305456   0.285426  -1.070    0.285    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 3.774 on 320 degrees of freedom\n",
       "Multiple R-squared:  0.9089,\tAdjusted R-squared:  0.908 \n",
       "F-statistic:  1064 on 3 and 320 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_amazon = lm(aprice ~ lprice + pages + width, data = df)\n",
    "summary(lm_amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, note that the full F-test has a very large F-statistic ($1064$), and very small p-value ($2.2 \\times 10^{-16}$, effectively zero). Typically, we should look at the full F-test first, to see if there is any evidence that any of the predictors are necesary in the model. Only after a significant full F-test should we look at an individual t-test.\n",
    "\n",
    "We note again that the t-test associated with `width` is not significant, suggesting that there is no evidence that the parameter associated with `width` is different from zero. \n",
    "\n",
    "But even though `pages` is significant, it seems clear that `lprice` is most strongly associated with `aprice` (pages predictor value is very close to 0. So, we might look at an F-test comparing the models:\n",
    "\n",
    "$$H_0: Y_i = \\beta_0 + \\beta_{lprice} \\left(lprice\\right) + \\varepsilon_i$$\n",
    "\n",
    "with \n",
    "\n",
    "$$H_1: \\text{number of pages or width (or both) should be included in the model. } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Res.Df</th><th scope=col>RSS</th><th scope=col>Df</th><th scope=col>Sum of Sq</th><th scope=col>F</th><th scope=col>Pr(&gt;F)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>322        </td><td>4846.160   </td><td>NA         </td><td>      NA   </td><td>      NA   </td><td>         NA</td></tr>\n",
       "\t<tr><td>320        </td><td>4557.841   </td><td> 2         </td><td>288.3194   </td><td>10.12126   </td><td>5.46791e-05</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " Res.Df & RSS & Df & Sum of Sq & F & Pr(>F)\\\\\n",
       "\\hline\n",
       "\t 322         & 4846.160    & NA          &       NA    &       NA    &          NA\\\\\n",
       "\t 320         & 4557.841    &  2          & 288.3194    & 10.12126    & 5.46791e-05\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Res.Df | RSS | Df | Sum of Sq | F | Pr(>F) |\n",
       "|---|---|---|---|---|---|\n",
       "| 322         | 4846.160    | NA          |       NA    |       NA    |          NA |\n",
       "| 320         | 4557.841    |  2          | 288.3194    | 10.12126    | 5.46791e-05 |\n",
       "\n"
      ],
      "text/plain": [
       "  Res.Df RSS      Df Sum of Sq F        Pr(>F)     \n",
       "1 322    4846.160 NA       NA        NA          NA\n",
       "2 320    4557.841  2 288.3194  10.12126 5.46791e-05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_amazon_reduced = lm(aprice ~ lprice, data = df)\n",
    "anova(lm_amazon_reduced, lm_amazon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the p-value associated with this partial F-test is small ($5.46791\\times 10^{-5} < \\alpha = 0.05$). This, we conclude that there is evidence that the reduced model is insufficient, and that we need at least one of the other predictors. We know that `width` is not statistically significant, and so we will only add back `pages`. This would leave us with the model \n",
    "\n",
    "$$Y_i = \\beta_0 + \\beta_{lprice} \\left(lprice\\right) + \\beta_{pages} \\left(pages\\right)  + \\varepsilon_i. $$\n",
    "\n",
    "\n",
    "\n",
    "Interestingly, F-tests can be used when comparing two models that differ only by one predictor. For example, comparing \n",
    "\n",
    "$$\\omega: Y_i = \\beta_0 + \\beta_{lprice} \\left(lprice\\right) + \\beta_{pages} \\left(pages\\right)  + \\varepsilon_i$$\n",
    "\n",
    "with\n",
    " \n",
    " $$\\Omega:Y_i = \\beta_0 + \\beta_{lprice} \\left(lprice\\right) + \\beta_{pages} \\left(pages\\right) + \\beta_{width} \\left(width\\right)  + \\varepsilon_i.$$\n",
    " \n",
    "Does the individual t-test and the F-test give consistent results? Let's check!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Res.Df</th><th scope=col>RSS</th><th scope=col>Df</th><th scope=col>Sum of Sq</th><th scope=col>F</th><th scope=col>Pr(&gt;F)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>321      </td><td>4574.153 </td><td>NA       </td><td>      NA </td><td>      NA </td><td>       NA</td></tr>\n",
       "\t<tr><td>320      </td><td>4557.841 </td><td> 1       </td><td>16.31249 </td><td>1.145279 </td><td>0.2853462</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " Res.Df & RSS & Df & Sum of Sq & F & Pr(>F)\\\\\n",
       "\\hline\n",
       "\t 321       & 4574.153  & NA        &       NA  &       NA  &        NA\\\\\n",
       "\t 320       & 4557.841  &  1        & 16.31249  & 1.145279  & 0.2853462\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Res.Df | RSS | Df | Sum of Sq | F | Pr(>F) |\n",
       "|---|---|---|---|---|---|\n",
       "| 321       | 4574.153  | NA        |       NA  |       NA  |        NA |\n",
       "| 320       | 4557.841  |  1        | 16.31249  | 1.145279  | 0.2853462 |\n",
       "\n"
      ],
      "text/plain": [
       "  Res.Df RSS      Df Sum of Sq F        Pr(>F)   \n",
       "1 321    4574.153 NA       NA        NA        NA\n",
       "2 320    4557.841  1 16.31249  1.145279 0.2853462"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = aprice ~ lprice + pages + width, data = df)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-19.3092  -1.7824  -0.0695   1.3374  22.9248 \n",
       "\n",
       "Coefficients:\n",
       "             Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  0.862994   1.573723   0.548    0.584    \n",
       "lprice       0.854834   0.017848  47.895  < 2e-16 ***\n",
       "pages       -0.006044   0.001348  -4.482 1.03e-05 ***\n",
       "width       -0.305456   0.285426  -1.070    0.285    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 3.774 on 320 degrees of freedom\n",
       "Multiple R-squared:  0.9089,\tAdjusted R-squared:  0.908 \n",
       "F-statistic:  1064 on 3 and 320 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_amazon_reduced2 = lm(aprice ~ lprice + pages, data = df)\n",
    "anova(lm_amazon_reduced2, lm_amazon)\n",
    "summary(lm_amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the p-value for the individual t-test for the parameter associated with `width`, and the p-value for this partial F-test are the same! This is not an accident, but a consequence of the relationship between the t-distribution and the F-distribution: if $X \\sim t(n)$ then $X^2 \\sim F_{1,n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = aprice ~ lprice + pages, data = df)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-19.0969  -1.8256  -0.0329   1.4436  23.3954 \n",
       "\n",
       "Coefficients:\n",
       "             Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -0.727973   0.516361  -1.410     0.16    \n",
       "lprice       0.844690   0.015127  55.841  < 2e-16 ***\n",
       "pages       -0.005824   0.001333  -4.369 1.69e-05 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 3.775 on 321 degrees of freedom\n",
       "Multiple R-squared:  0.9086,\tAdjusted R-squared:  0.908 \n",
       "F-statistic:  1595 on 2 and 321 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(lm_amazon_reduced2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
